# Phase 3c Embedding Bake-Off smoke configuration
experiment:
  name: "phase3c_embedding_bakeoff_smoke"
  output_dir: "results/embedding_bakeoff_smoke"
  per_run_metrics_file: "per_run_metrics.jsonl"
  summary_file: "summary_metrics.json"
  detailed_results_dir: "runs"

dataset:
  queries_path: "data/evaluation/gold/queries.jsonl"
  answers_path: "data/evaluation/gold/answers.jsonl"
  qrels_path: "data/evaluation/gold/qrels.tsv"
  document_source:
    markdown_dir: "docs/google_sites_guide"
    # For smoke metrics that align with markdown-only qrels, exclude ServiceNow
    # Set to null to avoid loading SN docs that can dominate retrieval
    servicenow_jsonl: null

runtime:
  max_queries: 3
  persist_responses: true

frozen:
  generator:
    provider: "llama_cpp"
    llm_name: "Qwen2.5-14B-Instruct"
    temperature: 0.0
    max_new_tokens: 128
    # Force a simple ChatML-style template without tool-calling branches
    chat_template: "configs/chat_templates/qwen_chatml_no_tools.jinja"
  index:
    type: "faiss"
    metric: "cosine"
  metrics:
    retrieval_k: 10

sweeps:
  embedding_models:
    - name: "bge-small-en-v1.5"
      source: "registry"
  chunk_size: [256]
  chunk_overlap: [32]
  top_k: [10]

judge:
  provider: "llama_cpp"
  llm_name: "Qwen2.5-14B-Instruct"
  # Keep judge on the same simple template for consistency
  chat_template: "configs/chat_templates/qwen_chatml_no_tools.jinja"
  prompt: |
    You are an expert RAG evaluator. Assess whether the generated answer is supported by the retrieved context.
    Score faithfulness on a scale of 1 (hallucinated) to 5 (fully grounded in the context).
    Return ONLY a JSON object with the following exact shape and no extra text:
    {"score": <integer 1-5>, "justification": "<one short sentence>"}

  faithfulness_threshold: 4
  hallucination_threshold: 2
